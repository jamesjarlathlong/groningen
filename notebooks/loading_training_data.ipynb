{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "import functools\n",
    "import data.helpers as helpers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from data.tile_creater import discretiser, grid_translation, get_pixel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "def plot_slice(tslice, label):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.set_size_inches(8,6)\n",
    "    ax = sns.heatmap(tslice, ax=ax, vmax=1000)\n",
    "    #ax.set_aspect('equal')\n",
    "    ax.scatter(label[0], label[1], marker='o', color = 'r', s=50)\n",
    "    sns.despine()\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('inspecttile', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fnames():\n",
    "    prefix = 'data/tiles_small/'\n",
    "    fnames = [prefix+i for i in os.listdir(prefix)\n",
    "             if i.split('.')[-1] == 'txt']\n",
    "    return fnames\n",
    "def parse_filename(fname):\n",
    "    no_prefix = fname.strip('data/tiles_small')\n",
    "    eventid, seq_id = no_prefix.split('_')\n",
    "    return eventid, int(seq_id.strip('.txt'))\n",
    "def get_label(metadata, filename):\n",
    "    eventid, seqid = parse_filename(filename)\n",
    "    label = metadata[eventid]\n",
    "    return label\n",
    "def _format_label(eventlat,eventlon, eventdepth,\n",
    "                  eventmag,eventid,topleft,\n",
    "                  size_x,size_y,numx,numy, nonzeros):\n",
    "    \"\"\"\n",
    "    Args: label is like (53.223, 6.949, 3.0, 0.502994544, 'knmi2018etrn',...)\n",
    "    (oneevent['eventlat'],oneevent['eventlon']\n",
    "            ,oneevent['eventdepth'], oneevent['magnitude']\n",
    "            ,oneevent['eventid'], topleft, sizex, sizey, numx,numy)\n",
    "    Returns: onehot tile encoding - numpy array\n",
    "    \"\"\"\n",
    "    xy = grid_translation(topleft,eventlat,eventlon)\n",
    "    plen_x, plen_y = get_pixel_lens(numx, numy, size_x, size_y)\n",
    "    n,m = discretiser(plen_x,plen_y,xy)\n",
    "    event_label_matrix = np.zeros((int(numy),int(numx)))\n",
    "    try:\n",
    "        event_label_matrix[m,n]=1.0\n",
    "        event_label_matrix_flatten = event_label_matrix.flatten()\n",
    "        nonzeroidx = np.nonzero(event_label_matrix_flatten)[0][0]\n",
    "        return (m,n)#nonzeroidx#event_label_matrix_flatten\n",
    "    except IndexError:\n",
    "        print('outside grid')\n",
    "        #earthquake was outside our grid if index error\n",
    "        return\n",
    "def format_frames(metadata,frames):\n",
    "    \"\"\"frames is a sequence of (eventid,seqnum,array)\"\"\"\n",
    "    frame_group = list(frames)[0:20]\n",
    "    label = metadata[frames[0][0]]\n",
    "    onehot = _format_label(*label)\n",
    "    tensor_frames = np.dstack([np.loadtxt(frame[2]) \n",
    "                               for frame in frame_group])\n",
    "    return tensor_frames, onehot\n",
    "def consistent_grouper(n, seq):\n",
    "    return (i for i in helpers.grouper(n, seq)\n",
    "            if len(i)==n)\n",
    "\n",
    "def gen(metadatafile,frames_per_eg, datafilenames):\n",
    "    with open(metadatafile) as f:\n",
    "        metadata = json.load(f)\n",
    "    loaded_files = ((*parse_filename(fname),fname)\n",
    "                    for fname in datafilenames) #np.loadtxt here in real execution\n",
    "    sorted_by_event = sorted(loaded_files, key=lambda x:x[0])\n",
    "    grouped_by_event = itertools.groupby(sorted_by_event, key = lambda x:x[0])\n",
    "    inorder_by_event = (sorted(g, key = lambda x:x[1]) for k,g in grouped_by_event)\n",
    "    window_groups_by_event = (helpers.grouper(frames_per_eg, group)\n",
    "                              for group in inorder_by_event)\n",
    "    formatted = (format_frames(metadata,framegroup) for framegroup\n",
    "                 in itertools.chain(*window_groups_by_event))\n",
    "    ingrid = ((tensor, label) for tensor, label\n",
    "             in formatted if label)\n",
    "    return ingrid\n",
    "    #for idx, row in x.iterrows():\n",
    "    #    data = np.asarray([row,\n",
    "    #                       row])\n",
    "    #    data = np.dstack((data,data, data))\n",
    "    #    label = y[idx]\n",
    "    #    yield data, label\n",
    "def train_input_fn(metadatafile, train_filenames, batch_size=20, repeat=1):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))#.map(namer)\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: gen(metadatafile,500, train_filenames)\n",
    "                                             ,output_types=(tf.int64, tf.int64)\n",
    "                                             ,output_shapes = (tf.TensorShape([30,60,20]),\n",
    "                                                               tf.TensorShape([2]))\n",
    "                                            )\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(100).repeat(repeat).batch(batch_size)\n",
    "    # Return the dataset.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    d,l = iterator.get_next()\n",
    "    return {'image':d}, l\n",
    "def test_input_fn(metadatafile, test_filenames, batch_size=1):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))#.map(namer)\n",
    "    print('testing: ', len(test_filenames))\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: gen(metadatafile,500, test_filenames)\n",
    "                                             ,output_types=(tf.int64, tf.int64)\n",
    "                                             ,output_shapes = (tf.TensorShape([30,60,20]),\n",
    "                                                               tf.TensorShape([2]))\n",
    "                                            )\n",
    "    # Return the dataset.\n",
    "    dataset=dataset.batch(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    d,l = iterator.get_next()\n",
    "    return {'image':d}, l\n",
    "\n",
    "def split_traintest(fnames, train_proportion):\n",
    "    eventgetter = lambda f: parse_filename(f)[0]\n",
    "    orderedbyevent = sorted(fnames, key = eventgetter) \n",
    "    groupedbyevent = [list(g) for k,g in\n",
    "                      itertools.groupby(orderedbyevent, key = eventgetter)]\n",
    "    numevents = len(groupedbyevent)\n",
    "    n_training = int(numevents*train_proportion)\n",
    "    train, test = groupedbyevent[0:n_training], groupedbyevent[n_training::]\n",
    "    return list(itertools.chain(*train)), list(itertools.chain(*test))\n",
    "def specificevent(fnames,event):\n",
    "    matching = [i for i in fnames if parse_filename(i)[0]==event]\n",
    "    return matching\n",
    "def justoneevent(fnames):\n",
    "    s = sorted(fnames, key = lambda f: parse_filename(f)[0], reverse=True)\n",
    "    grouped = itertools.groupby(s, key = lambda f: parse_filename(f)[0])\n",
    "    justone = next((list(g) for k,g in grouped))\n",
    "    return sorted(justone, key = lambda f: parse_filename(f)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_traintest(get_fnames(), 0.7)\n",
    "justone = specificevent(get_fnames(), 'knmi2017gpqn')#justoneevent(train)\n",
    "with tf.Session() as sess:\n",
    "    n = train_input_fn('data/tiles_small/metadata.json',justone)\n",
    "    i, label = sess.run(n)\n",
    "    print(label[0])\n",
    "    for idx in range(20):\n",
    "        sliced = i['image'][0,:,:,idx]\n",
    "        plot_slice(sliced, label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(labelmeta, train_fnames, test_fnames): \n",
    "    my_feature_columns = [tf.feature_column.numeric_column('image',shape=10*30*60)]\n",
    "    # Build 2 hidden layer DNN with 100, 100 units respectively.\n",
    "    classifier = tf.estimator.DNNRegressor(\n",
    "        feature_columns=my_feature_columns,\n",
    "        # The model must choose between 4 classes\n",
    "        label_dimension=2,\n",
    "        # Two hidden layers of 100 nodes each.\n",
    "        hidden_units=[10],\n",
    "        model_dir = 'tmp/small_tiles_debug',\n",
    "        config=tf.estimator.RunConfig().replace(save_summary_steps=1)\n",
    "        )\n",
    "    # Train the Model.\n",
    "    classifier.train(input_fn=lambda:train_input_fn(labelmeta,train_fnames,repeat=1000))\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:test_input_fn(labelmeta, test_fnames))\n",
    "    print([k for k in eval_result])\n",
    "    #print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:test_input_fn(labelmeta, test_fnames))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_traintest(get_fnames(), 0.7)\n",
    "justone = specificevent(get_fnames(), 'knmi2017gpqn')\n",
    "labelmeta = 'data/tiles_small/metadata.json'\n",
    "c = main(labelmeta, justone, justone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = c.predict(\n",
    "        input_fn=lambda:test_input_fn(labelmeta, justone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testeg= [i[1] for i in gen(labelmeta,10, justone)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlabel = [i.get('predictions') for i in pred]\n",
    "predlabel[0], testeg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "s,f =(0,2000)\n",
    "ax.scatter([i[0] for i in predlabel[s:f]], [i[1] for i in predlabel[s:f]], color ='b')\n",
    "for one, two in list(zip(predlabel, testeg))[s:f]:\n",
    "    ax.plot([one[0],two[0]], [one[1],two[1]], linestyle ='--', alpha=0.1)\n",
    "ax.scatter([i[0] for i in testeg[s:f]], [i[1] for i in testeg[s:f]], color ='r', s=50)\n",
    "sns.despine()\n",
    "ax.set_xlim([0,60])\n",
    "ax.set_ylim([0,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groningen",
   "language": "python",
   "name": "groningen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
